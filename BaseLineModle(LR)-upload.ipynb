{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700c250-ddcc-4d6b-a5c7-03d78f513d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    matthews_corrcoef, roc_curve, auc\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import interp\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"C:/Users\") # file path\n",
    "y = df[\"TMT\"]\n",
    "X = df.drop(columns=[\"TMT\"])\n",
    "\n",
    "# 5-fold CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)  \n",
    "\n",
    "# Store results for each fold\n",
    "acc_scores, f1_scores, recall_scores, prec_scores, mcc_scores, iter_counts = [], [], [], [], [], []\n",
    "\n",
    "# Store ROC curve data\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "print(\"ðŸ“Š Calibrated Logistic Regression (5-Fold CV) Performance:\\n\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    base_model = LogisticRegression(solver='liblinear', max_iter=100,random_state=42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "\n",
    "    calibrated_model = CalibratedClassifierCV(base_model, cv='prefit', method='sigmoid')   #'prefit'\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = calibrated_model.predict(X_test)\n",
    "    y_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # scores\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    prec_scores.append(precision_score(y_test, y_pred))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, y_pred))\n",
    "    iter_counts.append(base_model.n_iter_[0])\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    interp_tpr = interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.4, label=f'Fold {fold} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    print(f\"Fold {fold}: iter = {base_model.n_iter_[0]}, \"\n",
    "      f\"Accuracy = {acc_scores[-1]:.4f}, \"\n",
    "      f\"F1 = {f1_scores[-1]:.4f}, \"\n",
    "      f\"Recall = {recall_scores[-1]:.4f}, \"\n",
    "      f\"Precision = {prec_scores[-1]:.4f}, \"\n",
    "      f\"MCC = {mcc_scores[-1]:.4f}\")\n",
    "\n",
    "# Plot mean ROC curve\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(c) Baseline LR ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"C:/Users\",dpi=900, bbox_inches='tight') #Save file path\n",
    "plt.close()\n",
    "\n",
    "# Show mean and standard deviation results\n",
    "print(\"\\nðŸ“ˆ Average performanceï¼š\")\n",
    "print(f\"Accuracy : {np.mean(acc_scores):.4f} Â± {np.std(acc_scores):.4f}\")\n",
    "print(f\"F1 Score : {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "print(f\"Recall   : {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(prec_scores):.4f} Â± {np.std(prec_scores):.4f}\")\n",
    "print(f\"MCC      : {np.mean(mcc_scores):.4f} Â± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"\\nðŸŒ€ Average number of iterations: {np.mean(iter_counts):.1f} æ¬¡\")\n",
    "print(\"ðŸ“ˆ ROC plot saved as Calibrated_LR_5Fold_ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90455bd6-0107-4aac-b327-6e8e6fb17bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
